{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AnjanDutta/EEEM068/blob/main/Notebooks/Few_shot_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BZMAhZfNKkc"
   },
   "source": [
    "<H1 style=\"text-align: center\">EEEM068 - Applied Machine Learning</H1>\n",
    "<H1 style=\"text-align: center\">Workshop 06</H1>\n",
    "<H1 style=\"text-align: center\">Few-shot Learning Tutorial</H1>\n",
    "\n",
    "In this tutorial, we will work on algorithms that learn models which can quickly adapt to new classes and/or tasks with few samples. This area of machine learning is called _Meta-Learning_ that has a lot of application on Few-shot Learning.\n",
    "\n",
    "Meta-Learning offers solutions to these situations, and we will discuss one of the popular meta-learning algorithms: __Prototypical Networks (ProtoNet)__ ([Snell et al., 2017](https://arxiv.org/pdf/1703.05175.pdf)) which is applied for few-shot image classification task. We will focus on the task of few-shot classification where the training and test set have distinct sets of classes. For instance, we would train the model on the binary classifications of cats-birds and flowers-bikes, but during test time, the model would need to learn from 4 examples each the difference between dogs and otters, two classes we have not seen during training (Figure credit - [Lilian Weng](https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html)).\n",
    "\n",
    "<center width=\"100%\"><img src=\"https://lilianweng.github.io/posts/2018-11-30-meta-learning/few-shot-classification.png\" width=\"800px\"></center>\n",
    "\n",
    "For few-shot classification, we consider a task to distinguish between $M$ novel classes. Here, we would not only have novel classes, but also a completely different dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l44_ucnRNZBj"
   },
   "source": [
    "## Few-shot classification\n",
    "\n",
    "We start our implementation by discussing the dataset setup. In this notebook, we will use CIFAR-100; a description of CIFAR-100 can be found at [this link](https://www.cs.toronto.edu/~kriz/cifar.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RgYjXBMUUYlb"
   },
   "source": [
    "###CIFAR-100\n",
    "\n",
    "CIFAR-100 has 100 classes each with 600 images of size $32\\times 32$ pixels. Instead of splitting the training, validation, and test set over examples, we will split them over classes: we will use 80 classes for training, and 10 for validation, and 10 for testing. Our overall goal is to obtain a model that can distinguish between the 10 test classes with seeing very few examples. First, let's load the dataset and visualize some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wL8K4lA8NZBk"
   },
   "outputs": [],
   "source": [
    "# Loading CIFAR100 dataset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR100\n",
    "CIFAR_train_set = CIFAR100(root='./data/', train=True, download=True, transform=transforms.ToTensor())\n",
    "CIFAR_test_set = CIFAR100(root='./data/', train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3MkIMVfVRj-"
   },
   "source": [
    "###Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1SCLZkL3NZBk"
   },
   "outputs": [],
   "source": [
    "# Visualize some examples\n",
    "import torch\n",
    "import numpy as np\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('cividis')\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "NUM_IMAGES = 12\n",
    "CIFAR_images = torch.stack([CIFAR_train_set[np.random.randint(len(CIFAR_train_set))][0] for idx in range(NUM_IMAGES)], dim=0)\n",
    "img_grid = torchvision.utils.make_grid(CIFAR_images, nrow=6, normalize=True, pad_value=0.9)\n",
    "img_grid = img_grid.permute(1, 2, 0)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.title(\"Image examples of the CIFAR100 dataset\")\n",
    "plt.imshow(img_grid)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXHiP7oxNZBl"
   },
   "source": [
    "### Data preprocessing\n",
    "\n",
    "Next, we need to prepare the dataset in the training, validation and test split as mentioned before. The torchvision package gives us the training and test set as two separate dataset objects. The next code cells will merge the original training and test set, and then create the new train-val-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYyx54RZNZBl"
   },
   "outputs": [],
   "source": [
    "# Merging original training and test set\n",
    "CIFAR_all_images = np.concatenate([CIFAR_train_set.data, CIFAR_test_set.data], axis=0)\n",
    "CIFAR_all_targets = torch.LongTensor(CIFAR_train_set.targets + CIFAR_test_set.targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hc5AFIKANZBm"
   },
   "source": [
    "To have an easier time handling the dataset, we define our own, simple dataset class below. It takes a set of images, labels/targets, and image transformations, and returns the corresponding images and labels element-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C8_y2j00NZBm"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch.utils.data as data\n",
    "class ImageDataset(data.Dataset):\n",
    "    def __init__(self, imgs, targets, img_transform=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            imgs - Numpy array of shape [N,32,32,3] containing all images.\n",
    "            targets - PyTorch array of shape [N] containing all labels.\n",
    "            img_transform - A torchvision transformation that should be applied\n",
    "                            to the images before returning. If none, no transformation\n",
    "                            is applied.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.img_transform = img_transform\n",
    "        self.imgs = imgs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, target = self.imgs[idx], self.targets[idx]\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.img_transform is not None:\n",
    "            img = self.img_transform(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.imgs.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEadGq5cNZBm"
   },
   "source": [
    "Now, we can create the class splits. We will assign the classes randomly to training, validation and test, and use a 80%-10%-10% split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cFhJvnr2NZBn"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)           # Set seed for reproducibility\n",
    "classes = torch.randperm(100)  # Returns random permutation of numbers 0 to 99\n",
    "train_classes, val_classes, test_classes = classes[:80], classes[80:90], classes[90:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TaPD7FhoNZBn"
   },
   "source": [
    "To get an intuition of the validation and test classes, we print the class names below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GFfzwHrbNZBn"
   },
   "outputs": [],
   "source": [
    "# Printing validation and test classes\n",
    "idx_to_class = {val: key for key, val in CIFAR_train_set.class_to_idx.items()}\n",
    "print(\"Validation classes:\", [idx_to_class[c.item()] for c in val_classes])\n",
    "print(\"Test classes:\", [idx_to_class[c.item()] for c in test_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3P1GEcANZBn"
   },
   "source": [
    "As we can see, the classes have quite some variety and some classes might be easier to distinguish than others. For instance, in the test classes, 'pickup_truck' is the only vehicle while the classes 'mushroom', 'worm' and 'forest' might be harder to keep apart. Remember that we want to learn the classification of those ten classes from 80 other classes in our training set, and few examples from the actual test classes. We will experiment with the number of examples per class. Finally, we can create the training, validation and test dataset according to our split above. For this, we create dataset objects of our previously defined class `ImageDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sj01RJmaNZBo"
   },
   "outputs": [],
   "source": [
    "def dataset_from_labels(imgs, targets, class_set, **kwargs):\n",
    "    class_mask = (targets[:,None] == class_set[None,:]).any(dim=-1)\n",
    "    return ImageDataset(imgs=imgs[class_mask],\n",
    "                        targets=targets[class_mask],\n",
    "                        **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHeEGLqENZBo"
   },
   "source": [
    "As in our experiments before on CIFAR in Tutorial 5, 6 and 9, we normalize the dataset. Additionally, we use small augmentations during training to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BffwYy50NZBo"
   },
   "outputs": [],
   "source": [
    "# Pre-computed statistics from the new train set\n",
    "DATA_MEANS = torch.Tensor([0.5183975 , 0.49192241, 0.44651328])\n",
    "DATA_STD = torch.Tensor([0.26770132, 0.25828985, 0.27961241])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize(\n",
    "                                         DATA_MEANS, DATA_STD)\n",
    "                                     ])\n",
    "# For training, we add some augmentation.\n",
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomResizedCrop(\n",
    "                                          (32, 32), scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(\n",
    "                                          DATA_MEANS, DATA_STD)\n",
    "                                      ])\n",
    "\n",
    "train_set = dataset_from_labels(\n",
    "    CIFAR_all_images, CIFAR_all_targets, train_classes, img_transform=train_transform)\n",
    "val_set = dataset_from_labels(\n",
    "    CIFAR_all_images, CIFAR_all_targets, val_classes, img_transform=test_transform)\n",
    "test_set = dataset_from_labels(\n",
    "    CIFAR_all_images, CIFAR_all_targets, test_classes, img_transform=test_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-H1t9_KbNZBo"
   },
   "source": [
    "### Data sampling\n",
    "\n",
    "The strategy of how to use the available training data for learning few-shot adaptation is crucial in meta-learning. The meta-learning algorithms that we use here is based on the idea: simulate few-shot learning during training. Specifically, at each training step, we randomly select a small number of classes and sample a small number of examples for each class. This represents our few-shot training batch, which we also refer to as **support set**. Additionally, we sample a second set of examples from the same classes and refer to this batch as **query set**. Our training objective is to classify the query set correctly from seeing the support set and its corresponding labels.\n",
    "\n",
    "The following subsection summarizes the code that is needed to create such training batches. In PyTorch, we can specify the data sampling procedure by so-called `Sampler` ([documentation](https://pytorch.org/docs/stable/data.html#data-loading-order-and-sampler)). Samplers are iterable objects that return indices in the order in which the data elements should be sampled. In our previous notebooks, we usually used the option `shuffle=True` in the `data.DataLoader` objects which creates a sampler returning the data indices in random order. Here, we focus on samplers that return batches of indices that correspond to support and query set batches. Below, we implement such a sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_DfuafnkNZBo"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "class FewShotBatchSampler(object):\n",
    "\n",
    "    def __init__(self, dataset_targets, N_way, K_shot, include_query=False, shuffle=True, shuffle_once=False):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            dataset_targets - PyTorch tensor of the labels of the data elements.\n",
    "            N_way - Number of classes to sample per batch.\n",
    "            K_shot - Number of examples to sample per class in the batch.\n",
    "            include_query - If True, returns batch of size N_way*K_shot*2, which\n",
    "                            can be split into support and query set. Simplifies\n",
    "                            the implementation of sampling the same classes but\n",
    "                            distinct examples for support and query set.\n",
    "            shuffle - If True, examples and classes are newly shuffled in each\n",
    "                      iteration (for training)\n",
    "            shuffle_once - If True, examples and classes are shuffled once in\n",
    "                           the beginning, but kept constant across iterations\n",
    "                           (for validation)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dataset_targets = dataset_targets\n",
    "        self.N_way = N_way\n",
    "        self.K_shot = K_shot\n",
    "        self.shuffle = shuffle\n",
    "        self.include_query = include_query\n",
    "        if self.include_query:\n",
    "            self.K_shot *= 2\n",
    "        self.batch_size = self.N_way * self.K_shot  # Number of overall images per batch\n",
    "\n",
    "        # Organize examples by class\n",
    "        self.classes = torch.unique(self.dataset_targets).tolist()\n",
    "        self.num_classes = len(self.classes)\n",
    "        self.indices_per_class = {}\n",
    "        self.batches_per_class = {}  # Number of K-shot batches that each class can provide\n",
    "        for c in self.classes:\n",
    "            self.indices_per_class[c] = torch.where(self.dataset_targets == c)[0]\n",
    "            self.batches_per_class[c] = self.indices_per_class[c].shape[0] // self.K_shot\n",
    "\n",
    "        # Create a list of classes from which we select the N classes per batch\n",
    "        self.iterations = sum(self.batches_per_class.values()) // self.N_way\n",
    "        self.class_list = [c for c in self.classes for _ in range(self.batches_per_class[c])]\n",
    "        if shuffle_once or self.shuffle:\n",
    "            self.shuffle_data()\n",
    "        else:\n",
    "            # For testing, we iterate over classes instead of shuffling them\n",
    "            sort_idxs = [i+p*self.num_classes for i,\n",
    "                         c in enumerate(self.classes) for p in range(self.batches_per_class[c])]\n",
    "            self.class_list = np.array(self.class_list)[np.argsort(sort_idxs)].tolist()\n",
    "\n",
    "    def shuffle_data(self):\n",
    "        # Shuffle the examples per class\n",
    "        for c in self.classes:\n",
    "            perm = torch.randperm(self.indices_per_class[c].shape[0])\n",
    "            self.indices_per_class[c] = self.indices_per_class[c][perm]\n",
    "        # Shuffle the class list from which we sample. Note that this way of shuffling\n",
    "        # does not prevent to choose the same class twice in a batch. However, for\n",
    "        # training and validation, this is not a problem.\n",
    "        random.shuffle(self.class_list)\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Shuffle data\n",
    "        if self.shuffle:\n",
    "            self.shuffle_data()\n",
    "\n",
    "        # Sample few-shot batches\n",
    "        start_index = defaultdict(int)\n",
    "        for it in range(self.iterations):\n",
    "            class_batch = self.class_list[it*self.N_way:(it+1)*self.N_way]  # Select N classes for the batch\n",
    "            index_batch = []\n",
    "            for c in class_batch:  # For each class, select the next K examples and add them to the batch\n",
    "                index_batch.extend(self.indices_per_class[c][start_index[c]:start_index[c]+self.K_shot])\n",
    "                start_index[c] += self.K_shot\n",
    "            if self.include_query:  # If we return support+query set, sort them so that they are easy to split\n",
    "                index_batch = index_batch[::2] + index_batch[1::2]\n",
    "            yield index_batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1f3dYkplNZBp"
   },
   "source": [
    "Note that this sampler eventually allows the sampling of batches where we use a class twice due to a simpler shuffling function (`shuffle_data`). In other words, during training or validation, if we sample batches for a 5-class 4-shot training setting, it can occasionally happen that we get a batch where 2 of the 5 classes are identical. Since we have 80 classes to choose from, this however happens relatively rarely. Further, it actually does not constitute any issue if the code for the meta-learning methods support variable number of classes and shots per class. Nonetheless, when the number of classes/tasks is smaller, it is recommended to replace the `shuffle_data` method with a sampler that prevents picking the same class twice in a batch.\n",
    "\n",
    "Now, we can create our intended data loaders by passing an object of `FewShotBatchSampler` as `batch_sampler=...` input to the PyTorch data loader object. For our experiments, we will use a 5-class 4-shot training setting. This means that each support set contains 5 classes with 4 examples each, i.e., 20 images overall. Usually, it is good to keep the number of shots equal to the number that you aim to test on. However, we will experiment later with a different number of shots, and hence, we pick 4 as a compromise for now. To get the best-performing model, it is recommended to consider the number of training shots as hyperparameters in a grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GOuty3JnNZBp"
   },
   "outputs": [],
   "source": [
    "N_WAY = 5\n",
    "K_SHOT = 4\n",
    "train_loader = data.DataLoader(train_set, batch_sampler=FewShotBatchSampler(train_set.targets,\n",
    "                                                                            include_query=True,\n",
    "                                                                            N_way=N_WAY,\n",
    "                                                                            K_shot=K_SHOT,\n",
    "                                                                            shuffle=True),\n",
    "                               num_workers=2)\n",
    "val_loader = data.DataLoader(val_set, batch_sampler=FewShotBatchSampler(val_set.targets,\n",
    "                                                                        include_query=True,\n",
    "                                                                        N_way=N_WAY,\n",
    "                                                                        K_shot=K_SHOT,\n",
    "                                                                        shuffle=False,\n",
    "                                                                        shuffle_once=True),\n",
    "                             num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gY63ceoeNZBp"
   },
   "source": [
    "For simplicity, we implemented the sampling of a support and query set as sampling a support set with twice the number of examples. After sampling a batch from the data loader, we need to split it into a support and query set. We can summarize this step in the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gK7yqCm_NZBp"
   },
   "outputs": [],
   "source": [
    "def split_batch(imgs, targets):\n",
    "    support_imgs, query_imgs = imgs.chunk(2, dim=0)\n",
    "    support_targets, query_targets = targets.chunk(2, dim=0)\n",
    "    return support_imgs, query_imgs, support_targets, query_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVhlRz4YNZBq"
   },
   "source": [
    "Finally, to ensure that our implementation of the data sampling process is correct, we can sample a batch and visualize its support and query set. What we would like to see is that the support and query set have the same classes, but distinct examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GAMdBnZkNZBq"
   },
   "outputs": [],
   "source": [
    "imgs, targets = next(iter(val_loader))  # We use the validation set since it does not apply augmentations\n",
    "support_imgs, query_imgs, _, _ = split_batch(imgs, targets)\n",
    "support_grid = torchvision.utils.make_grid(support_imgs, nrow=K_SHOT, normalize=True, pad_value=0.9)\n",
    "support_grid = support_grid.permute(1, 2, 0)\n",
    "query_grid = torchvision.utils.make_grid(query_imgs, nrow=K_SHOT, normalize=True, pad_value=0.9)\n",
    "query_grid = query_grid.permute(1, 2, 0)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 5))\n",
    "ax[0].imshow(support_grid)\n",
    "ax[0].set_title(\"Support set\")\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(query_grid)\n",
    "ax[1].set_title(\"Query set\")\n",
    "ax[1].axis('off')\n",
    "plt.suptitle(\"Few Shot Batch\", weight='bold')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oX8uyj36NZBq"
   },
   "source": [
    "As we can see, the support and query set have the same five classes, but different examples. The models will be tasked to classify the examples in the query set by learning from the support set and its labels. With the data sampling in place, we can now start to implement our first meta-learning model: Prototypical Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwlzlGh7Cmyv"
   },
   "source": [
    "## Average Meter\n",
    "\n",
    "It is a simple class for keeping training statistics, such as losses and accuracies etc. The .val field usually holds the statistics for the current batch, whereas the .avg field hold statistics for the current epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k4-m0xTUCusH"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yupfQmhNZBq"
   },
   "source": [
    "## Prototypical Networks (ProtoNet)\n",
    "\n",
    "The Prototypical Network (ProtoNet) is a metric-based meta-learning algorithm that operates similarly to the nearest neighbor classification. Metric-based meta-learning methods classify a new example $\\mathbf{x}$ based on some distance function $d_{\\varphi}$ between $x$ and all elements in the support set. ProtoNets implements this idea with the concept of prototypes in a learned feature space. First, ProtoNet uses an embedding function $f_{\\theta}$ to encode each input in the support set into a $L$-dimensional feature vector. Next, for each class $c$, we collect the feature vectors of all examples with label $c$ and average their feature vectors. Formally, we can define this as:\n",
    "\n",
    "$$\\mathbf{v}_c=\\frac{1}{|S_c|}\\sum_{(\\mathbf{x}_i,y_i)\\in S_c}f_{\\theta}(\\mathbf{x}_i)$$\n",
    "\n",
    "where $S_c$ is the part of the support set $S$ for which $y_i=c$, and $\\mathbf{v}_c$ represents the _prototype_ of class $c$. The prototype calculation is visualized below for a 2-dimensional feature space and 3 classes (Figure credit - [Snell et al.](https://arxiv.org/pdf/1703.05175.pdf)). The colored dots represent encoded support elements with the color-corresponding class labels, and the black dots next to the class label are the averaged prototypes.\n",
    "\n",
    "<center width=\"100%\"><img src=\"https://github.com/AnjanDutta/SharedFigures/blob/main/protonet_classification.svg?raw=true\" width=\"300px\"></center>\n",
    "\n",
    "Based on these prototypes, we want to classify a new example. Remember that since we want to learn the encoding function $f_{\\theta}$, this classification must be differentiable, and hence, we need to define a probability distribution across classes. For this, we will make use of the distance function $d_{\\varphi}$: the closer a new example $\\mathbf{x}$ is to a prototype $\\mathbf{v}_c$, the higher the probability for $\\mathbf{x}$ belonging to class $c$. Formally, we can simply use a softmax over the distances of $\\mathbf{x}$ to all class prototypes:\n",
    "\n",
    "$$p(y=c\\vert\\mathbf{x})=\\text{softmax}(-d_{\\varphi}(f_{\\theta}(\\mathbf{x}), \\mathbf{v}_c))=\\frac{\\exp\\left(-d_{\\varphi}(f_{\\theta}(\\mathbf{x}), \\mathbf{v}_c)\\right)}{\\sum_{c'\\in \\mathcal{C}}\\exp\\left(-d_{\\varphi}(f_{\\theta}(\\mathbf{x}), \\mathbf{v}_{c'})\\right)}$$\n",
    "\n",
    "Note that the negative sign is necessary since we want to increase the probability for close-by vectors and have a low probability for distant vectors. We train the network $f_{\\theta}$ based on the cross-entropy error of the training query set examples. Thereby, the gradient flows through both the prototypes $\\mathbf{v}_c$ and the query set encodings $f_{\\theta}(\\mathbf{x})$. For the distance function $d_{\\varphi}$, we can choose any function as long as it is differentiable concerning both of its inputs. The most common function, which we also use here, is the squared euclidean distance, but there have been several works on different distance functions as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsf9wakxNZBq"
   },
   "source": [
    "### ProtoNet implementation\n",
    "\n",
    "Now that we know how a ProtoNet works in principle, let's look at how we can apply it to our specific problem of few-shot image classification, and implement it below. First, we need to define the encoder function $f_{\\theta}$. Since we work with CIFAR images, we can take a look back at Tutorial 5 where we compared common Computer Vision architectures, and choose one of the best-performing ones. Here, we go with a DenseNet since it is in general more parameter efficient than ResNet. Luckily, we do not need to implement DenseNet ourselves again and can rely on torchvision's model package instead. We use common hyperparameters of 64 initial feature channels, add 32 per block, and use a bottleneck size of 64 (i.e. 2 times the growth rate). We use 4 stages of 6 layers each, which results in overall about 1 million parameters. Note that the torchvision package assumes that the last layer is used for classification and hence calls its output size `num_classes`. However, we can instead just use it as the feature space of ProtoNet and choose an arbitrary dimensionality. We will use the same network for other algorithms in this notebook to ensure a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GT4zH1QlNZBr"
   },
   "outputs": [],
   "source": [
    "def get_convnet(output_size):\n",
    "    convnet = torchvision.models.DenseNet(growth_rate=32,\n",
    "                                          block_config=(6, 6, 6, 6),\n",
    "                                          bn_size=2,\n",
    "                                          num_init_features=64,\n",
    "                                          num_classes=output_size)  # Output dimensionality\n",
    "    return convnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kD9MZYDiNZBr"
   },
   "source": [
    "Next, we can look at implementing ProtoNet. The first step during training is to encode all images in a batch with our network. Next, we calculate the class prototypes from the support set (function `calculate_prototypes`), and classify the query set examples according to the prototypes (function `classify_feats`). Keep in mind that we use the data sampling described before, such that the support and query set are stacked together in the batch. Thus, we use our previously defined function `split_batch` to split them apart. The full code can be found below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zh31B5RbNZBr"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ProtoNet(nn.Module):\n",
    "\n",
    "    def __init__(self, proto_dim, device):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            proto_dim - Dimensionality of prototype feature space\n",
    "            device - Device: cuda or cpu\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = get_convnet(output_size=proto_dim)\n",
    "        self.device = device\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_prototypes(features, targets):\n",
    "        # Given a stack of features vectors and labels, return class prototypes\n",
    "        # features - shape [N, proto_dim], targets - shape [N]\n",
    "        classes, _ = torch.unique(targets).sort()  # Determine which classes we have\n",
    "        prototypes = []\n",
    "        for c in classes:\n",
    "            p = features[torch.where(targets == c)[0]].mean(dim=0)  # Average class feature vectors\n",
    "            prototypes.append(p)\n",
    "        prototypes = torch.stack(prototypes, dim=0)\n",
    "        # Return the 'classes' tensor to know which prototype belongs to which class\n",
    "        return prototypes, classes\n",
    "\n",
    "    def classify_feats(self, prototypes, classes, feats, targets):\n",
    "        # Classify new examples with prototypes and return classification error\n",
    "        dist = torch.pow(prototypes[None, :] - feats[:, None], 2).sum(dim=2)  # Squared euclidean distance\n",
    "        preds = F.log_softmax(-dist, dim=1)\n",
    "        labels = (classes[None, :] == targets[:, None]).long().argmax(dim=-1)\n",
    "        acc = (preds.argmax(dim=1) == labels).float().mean()\n",
    "        return preds, labels, acc\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Determine training loss for a given support and query set\n",
    "        imgs, targets = batch\n",
    "        imgs, targets = imgs.to(self.device), targets.to(self.device)\n",
    "        features = self.model(imgs)  # Encode all images of support and query set\n",
    "        support_feats, query_feats, support_targets, query_targets = split_batch(features, targets)\n",
    "        prototypes, classes = self.calculate_prototypes(support_feats, support_targets)\n",
    "        preds, labels, acc = self.classify_feats(prototypes, classes, query_feats, query_targets)\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.calculate_loss(batch, mode=\"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _ = self.calculate_loss(batch, mode=\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtu06MhkNZBr"
   },
   "source": [
    "For validation, we use the same principle as training and sample support and query sets from the hold-out 10 classes. However, this gives us noisy scores depending on which query sets are chosen to which support sets. This is why we will use a different strategy during testing. For validation, our training strategy is sufficient since it is much faster than testing, and gives a good estimate of the training generalization as long as we keep the support-query sets constant across validation iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uW3ENBYpVUt8"
   },
   "source": [
    "## Train and Test Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1uTNiJ6NZBr"
   },
   "source": [
    "### Training\n",
    "\n",
    "The following training code is very similar to the previous tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vPoWL9NqNZBs"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "##define train function\n",
    "def train(model, device, train_loader, optimizer):\n",
    "    # meter\n",
    "    loss = AverageMeter()\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    tk0 = tqdm(train_loader, total=int(len(train_loader)))\n",
    "    for batch_idx, data in enumerate(tk0):\n",
    "        # after fetching the data transfer the model to the\n",
    "        # compute the forward pass\n",
    "        # it can also be achieved by model.forward(data)\n",
    "        loss_this = model(data)\n",
    "        # initialize the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        # compute the backward pass\n",
    "        loss_this.backward()\n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "        # update the loss meter\n",
    "        loss.update(loss_this.item(), data[1].shape[0])\n",
    "    print('Train: Average loss: {:.4f}\\n'.format(loss.avg))\n",
    "    return loss.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNX07CcGNZBt"
   },
   "source": [
    "### Testing\n",
    "\n",
    "Our goal of meta-learning is to obtain a model that can quickly adapt to a new task, or in this case, new classes to distinguish between. To test this, we will use our trained ProtoNet and adapt it to the 10 test classes. Thereby, we pick $k$ examples per class from which we determine the prototypes and test the classification accuracy on all other examples. This can be seen as using the $k$ examples per class as a support set, and the rest of the dataset as a query set. We iterate through the dataset such that each example has been once included in a support set. The average performance across all support sets tells us how well we can expect ProtoNet to perform when seeing only $k$ examples per class. During training, we used $k=4$. In testing, we will experiment with $k=\\{2,4,8,16,32\\}$ to get a better sense of how $k$ influences the results. We would expect that we achieve higher accuracies the more examples we have in the support set, but we don't know how it scales. Hence, let's first implement a function that executes the testing procedure for a given $k$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQHTJaAENZBt"
   },
   "outputs": [],
   "source": [
    "from statistics import mean, stdev\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, dataset, data_feats=None, k_shot=4):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "        model - Pretrained ProtoNet model\n",
    "        dataset - The dataset on which the test should be performed.\n",
    "                  Should be instance of ImageDataset\n",
    "        data_feats - The encoded features of all images in the dataset.\n",
    "                     If None, they will be newly calculated, and returned\n",
    "                     for later usage.\n",
    "        k_shot - Number of examples per class in the support set.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    num_classes = dataset.targets.unique().shape[0]\n",
    "    exmps_per_class = dataset.targets.shape[0]//num_classes  # We assume uniform example distribution here\n",
    "\n",
    "    # The encoder network remains unchanged across k-shot settings. Hence, we only need\n",
    "    # to extract the features for all images once.\n",
    "    if data_feats is None:\n",
    "        # Dataset preparation\n",
    "        dataloader = data.DataLoader(dataset, batch_size=128, num_workers=2, shuffle=False, drop_last=False)\n",
    "\n",
    "        img_features = []\n",
    "        img_targets = []\n",
    "        for imgs, targets in tqdm(dataloader, \"Extracting image features\", leave=False):\n",
    "            imgs = imgs.to(device)\n",
    "            feats = model.model(imgs)\n",
    "            img_features.append(feats.detach().cpu())\n",
    "            img_targets.append(targets)\n",
    "        img_features = torch.cat(img_features, dim=0)\n",
    "        img_targets = torch.cat(img_targets, dim=0)\n",
    "        # Sort by classes, so that we obtain tensors of shape [num_classes, exmps_per_class, ...]\n",
    "        # Makes it easier to process later\n",
    "        img_targets, sort_idx = img_targets.sort()\n",
    "        img_targets = img_targets.reshape(num_classes, exmps_per_class).transpose(0, 1)\n",
    "        img_features = img_features[sort_idx].reshape(num_classes, exmps_per_class, -1).transpose(0, 1)\n",
    "    else:\n",
    "        img_features, img_targets = data_feats\n",
    "\n",
    "    # We iterate through the full dataset in two manners. First, to select the k-shot batch.\n",
    "    # Second, the evaluate the model on all other examples\n",
    "    accuracies = []\n",
    "    for k_idx in tqdm(range(0, img_features.shape[0], k_shot), \"Evaluating prototype classification\", leave=False):\n",
    "        # Select support set and calculate prototypes\n",
    "        k_img_feats, k_targets = img_features[k_idx:k_idx+k_shot].flatten(0,1), img_targets[k_idx:k_idx+k_shot].flatten(0,1)\n",
    "        prototypes, proto_classes = model.calculate_prototypes(k_img_feats, k_targets)\n",
    "        # Evaluate accuracy on the rest of the dataset\n",
    "        batch_acc = 0\n",
    "        for e_idx in range(0, img_features.shape[0], k_shot):\n",
    "            if k_idx == e_idx:  # Do not evaluate on the support set examples\n",
    "                continue\n",
    "            e_img_feats, e_targets = img_features[e_idx:e_idx+k_shot].flatten(0,1), img_targets[e_idx:e_idx+k_shot].flatten(0,1)\n",
    "            _, _, acc = model.classify_feats(prototypes, proto_classes, e_img_feats, e_targets)\n",
    "            batch_acc += acc.item()\n",
    "        batch_acc /= img_features.shape[0]//k_shot-1\n",
    "        accuracies.append(batch_acc)\n",
    "\n",
    "    return (mean(accuracies), stdev(accuracies)), (img_features, img_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMX0EMPymGkY"
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76wC9vpfmPzx"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMupwFzANZBs"
   },
   "source": [
    "Below is the call for our ProtoNet. We use a 64-dimensional feature space. Larger feature spaces showed to give noisier results since the squared euclidean distance becomes proportionally larger in expectation, and smaller feature spaces might not allow for enough flexibility. We recommend to load the pre-trained model here at first, but feel free to play around with the hyperparameters yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRJp15DJENAf"
   },
   "outputs": [],
   "source": [
    "# device: cuda (gpu) or cpu\n",
    "device = \"cuda\"\n",
    "# define the model\n",
    "model = ProtoNet(proto_dim=64, device=device) # proto_dim = 64, device = cuda\n",
    "# map to device, `model.cuda()` will also do the same job\n",
    "model = model.to(device)\n",
    "# make the parameters trainable\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_xfc3WvmS2u"
   },
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0LkCerxONZBs"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "## some hyperparameters related to optimizer\n",
    "learning_rate = 0.0001\n",
    "weight_decay = 0.0005\n",
    "# define optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQStkoOAVoFF"
   },
   "source": [
    "## Training Loop\n",
    "After implementing the model and other necessary components, we can start training it. It would be good to train the model for 200 epochs, but that will take 6 to 7 hours. So for brevity, we will train it only for 10 epochs which will take about 15 to 20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9qaNMid4VnAd"
   },
   "outputs": [],
   "source": [
    "num_epoch = 10\n",
    "for epoch in range(1, num_epoch + 1):\n",
    "    epoch_loss = train(model, device, train_loader, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0sOOgcKYNZBt"
   },
   "source": [
    "Testing ProtoNet is relatively quick if we have processed all images once. Hence, we can do in this notebook very efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ky4hPf5FNZBt"
   },
   "outputs": [],
   "source": [
    "protonet_accuracies = dict()\n",
    "data_feats = None\n",
    "for k in [2, 4, 8, 16, 32]:\n",
    "    protonet_accuracies[k], data_feats = test(model, test_set, data_feats=data_feats, k_shot=k)\n",
    "    print(f\"Accuracy for k={k}: {100.0*protonet_accuracies[k][0]:4.2f}% (+-{100*protonet_accuracies[k][1]:4.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDn02ZjoNZBu"
   },
   "source": [
    "Before discussing the results above, let's first plot the accuracies over number of examples in the support set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TTBqs6LzNZBu"
   },
   "outputs": [],
   "source": [
    "def plot_few_shot(acc_dict, name, color=None, ax=None):\n",
    "    sns.set()\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1,1,figsize=(5,3))\n",
    "    ks = sorted(list(acc_dict.keys()))\n",
    "    mean_accs = [acc_dict[k][0] for k in ks]\n",
    "    std_accs = [acc_dict[k][1] for k in ks]\n",
    "    ax.plot(ks, mean_accs, marker='o', markeredgecolor='k', markersize=6, label=name, color=color)\n",
    "    ax.fill_between(ks, [m-s for m,s in zip(mean_accs, std_accs)], [m+s for m,s in zip(mean_accs, std_accs)], alpha=0.2, color=color)\n",
    "    ax.set_xticks(ks)\n",
    "    ax.set_xlim([ks[0]-1, ks[-1]+1])\n",
    "    ax.set_xlabel(\"Number of shots per class\", weight='bold')\n",
    "    ax.set_ylabel(\"Accuracy\", weight='bold')\n",
    "    if len(ax.get_title()) == 0:\n",
    "        ax.set_title(\"Few-Shot Performance \" + name, weight='bold')\n",
    "    else:\n",
    "        ax.set_title(ax.get_title() + \" and \" + name, weight='bold')\n",
    "    ax.legend()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KvA6FZLjNZBu"
   },
   "outputs": [],
   "source": [
    "ax = plot_few_shot(protonet_accuracies, name=\"ProtoNet\", color=\"C1\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQKKjk6UNZBu"
   },
   "source": [
    "As we initially expected, the performance of ProtoNet indeed increases the more samples we have. However, even with just two samples per class, we classify almost half of the images correctly, which is well above random accuracy (10%). The curve shows an exponentially dampend trend, meaning that adding 2 extra examples to $k=2$ has a much higher impact than adding 2 extra samples if we already have $k=16$. Nonetheless, we can say that ProtoNet adapts fairly well to new classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8aZ5PCDNZB0"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we have discussed meta-learning algorithms that learn to adapt to new classes and/or tasks with just a few samples. We have implemented Prototypical Network (ProtoNet). On the few-shot image classification task of CIFAR100, ProtoNet showed to perform reasonably well. It will also be interesting to try and implement other models such as, __Model-Agnostic Meta-Learning / MAML__ ([Finn et al., 2017](http://proceedings.mlr.press/v70/finn17a.html)), and __Proto-MAML__ ([Triantafillou et al., 2020](https://openreview.net/pdf?id=rkgAGAVKPr)) for the same task."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMWwXxxkqd/WDNkXdetafSw",
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
