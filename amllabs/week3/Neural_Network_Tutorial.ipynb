{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ECMM426/ECMM441 - Neural Networks.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnjanDutta/EEEM068/blob/main/Notebooks/Neural_Network_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_5Tkl9SzhjN",
        "pycharm": {}
      },
      "source": [
        "<H1 style=\"text-align: center\">EEEM068 - Applied Machine Learning</H1>\n",
        "<H1 style=\"text-align: center\">Workshop 03</H1>\n",
        "<H1 style=\"text-align: center\">Neural Network Tutorial</H1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8f95r3cyd1S",
        "pycharm": {}
      },
      "source": [
        "## Introduction\n",
        "In this workshop, we will implement a multi-layered perceptron (MLP) to classify digits. Specifically we will consider the MNIST dataset for training and testing our model. For this workshop, we are going to use [PyTorch](https://pytorch.org/), the cutting-edge deep learning framework to complete our task."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST Dataset"
      ],
      "metadata": {
        "id": "Z4ls6RhlRBeh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LT-4QzaoAxQX",
        "pycharm": {}
      },
      "source": [
        "The MNIST dataset contains 70000 black and white, hand-written digits (hence 10 classes or categories: 0, 1, ..., 9) images that are of size 28x28 pixels. This is a dataset that is typically used for demonstrations of machine learning models, and as a first dataset to test new types of models. The dataset is further split into train, validation and test sets. More details on the MNIST dataset can be found here: http://yann.lecun.com/exdb/mnist/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset class\n",
        "Torchvision (a part of the bigger PyTorch project/package) provides many built-in datasets in the `torchvision.datasets` module, as well as utility classes for building your own datasets. All datasets are subclasses of `torch.utils.data.Dataset` i.e, they have `__getitem__` and `__len__` methods implemented. Hence, they can all be passed to a `torch.utils.data.DataLoader` which can load multiple samples in parallel using `torch.multiprocessing` workers. More details on built-in datasets could be found in the [documentation](https://pytorch.org/vision/stable/datasets.html#).\n",
        "\n",
        "MNIST dataset is one of the many built-in datasets available via the torchvision package. In this workshop, we will use that MNIST dataset from `torchvision.datasets`.\n",
        "\n",
        "**Note:** If a dataset is not available within the built-in collection, it has to be implemented from scratch, which we will discuss in a future workshop."
      ],
      "metadata": {
        "id": "mtzBnkovYSe5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization\n",
        "Lets download the MNIST dataset from PyTorch's collection of dataset and plot some digits for visualization."
      ],
      "metadata": {
        "id": "iw0esdY8Q6ul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets\n",
        "mnist_train = list(datasets.MNIST('data/', train=True, download=True))\n",
        "# plot the first 64 images from the MNIST training data\n",
        "for k, (image, label) in enumerate(mnist_train[:64]):\n",
        "    plt.subplot(8, 8, k+1)\n",
        "    plt.imshow(image, cmap='gray')"
      ],
      "metadata": {
        "id": "nzS4ArD5R9Eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image transformations"
      ],
      "metadata": {
        "id": "n_44OYy7rOwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before inputing an image to a neural network, it is often needed to transform or process the image into a specific form. For example, an image needs to be converted into numerical pixels features form before inputting them into a network. The `torchvision.transforms` module of `torchvision` provides such common image transformations. They can be chain together using `Compose`. More details on image transformation functions using `Torchvision` can be found in the [documentation](https://pytorch.org/vision/stable/transforms.html)."
      ],
      "metadata": {
        "id": "nij9MaR8rSJX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Image to Tensor"
      ],
      "metadata": {
        "id": "j_SQfdTvVWdm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before inputing images to a neural network, images should be transformed into tensors. PyTorch provides `transforms.ToTensor()` function for us to convert an image into numerical pixel features. The tensor still preserves the 2D geometry of the image, i.e. we still get a matrix of 1x28x28 shape. More details can be found in its [documentation](https://pytorch.org/vision/main/generated/torchvision.transforms.ToTensor.html#totensor)."
      ],
      "metadata": {
        "id": "usRBTdsFVDXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "# transform the image data type to a 28x28 matrix of numbers\n",
        "# convert the last image we saw into a tensor\n",
        "img_tensor = transforms.ToTensor()(image)\n",
        "\n",
        "# print the shape of the resulting tensor\n",
        "img_tensor.shape"
      ],
      "metadata": {
        "id": "4-kpiOJjVKLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the values of img_tensor\n",
        "print(img_tensor)"
      ],
      "metadata": {
        "id": "Q2phSkKDXtre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Normalization"
      ],
      "metadata": {
        "id": "dY11Q38wtALK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalization is a technique often applied as part of data preparation for machine learning. The goal of normalization is to change the values of numeric rows / columns in the dataset to a common scale, without distorting differences in the ranges of values. For machine learning, every dataset does not require normalization. It is required only when features have different ranges. In neural network, it is a good practice to work with normalized data. PyTorch provides the `transforms.Normalize()` function which normalises the input. Please check its [documentation](https://pytorch.org/vision/main/generated/torchvision.transforms.Normalize.html#normalize) for more details."
      ],
      "metadata": {
        "id": "LuS_PK4StI4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Combination\n",
        "Any number of transformations can be chained together using `Compose`. More details are in the [documentation](https://pytorch.org/vision/main/generated/torchvision.transforms.Compose.html#compose)."
      ],
      "metadata": {
        "id": "oPbey6KLtlZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define how image should be transformed\n",
        "image_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.1307,), (0.3081,))])"
      ],
      "metadata": {
        "id": "dhJhjrC5tmUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image transformation via dataset class"
      ],
      "metadata": {
        "id": "ZD2ISOJHt-p7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combination of a set of transformations can be applied via the dataset class by passing the transformations to the `tranform` argument of the dataset class."
      ],
      "metadata": {
        "id": "4hlJhvGJm30B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Since MNIST dataset is already implemented within 'torchvision.datasets', we\n",
        "# don't need to implement seperate dataset for MNIST. Below is how we can use\n",
        "# the MNIST dataset respectively for training and testing\n",
        "train_dataset = datasets.MNIST('data/', train=True, download=True,\n",
        "                               transform=image_transform)\n",
        "test_dataset = datasets.MNIST('data/', train=False, transform=image_transform)"
      ],
      "metadata": {
        "id": "oeu9qlebY0W2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data loader\n",
        "\n",
        "Once we have the dataset ready, we can create the dataloader as follows. It is recommended to use smaller batch size for training and larger batch size during test time, because during training lot of intermediate variables need to be kept in the GPU memory for processing."
      ],
      "metadata": {
        "id": "MFH4OTjfuGWP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7KkXL5OA8WX",
        "pycharm": {},
        "collapsed": true
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "## Create dataloader, in PyTorch, we feed the trainer data with dataloader\n",
        "## We create dataloader with dataset from torchvision, and we don't have to\n",
        "## download it seperately, all automatically done\n",
        "\n",
        "# Define batch size, batch size is how much data you feed for training in one iteration\n",
        "batch_size_train = 256 # We use a small batch size here for training\n",
        "batch_size_test = 1024 #\n",
        "\n",
        "#data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                           batch_size=batch_size_train,\n",
        "                                           shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                          batch_size=batch_size_test,\n",
        "                                          shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUWGhlsMCYZD",
        "pycharm": {}
      },
      "source": [
        "## Model\n",
        "We need to define trainable layers and put them inside a model. Have a look on the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#module) of `nn.Module` and read more about different layers and functionalities of PyTorch there. In this model, we are going to use the following functions or modules:\n",
        "\n",
        "* `nn.Sequential()`: It is a sequential container. Modules will be added to it in the order they are passed in the constructor. Please check the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential) for more details.\n",
        "\n",
        "* `nn.Linear()`: It is a module that applies a linear transformation to the incoming data. More details can be found in its [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#linear).\n",
        "\n",
        "* `nn.ReLU()`: It is also a module that applies element-wise the rectified linear unit function. Its [documentation](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#relu) can explain more.\n",
        "\n",
        "* `nn.Dropout()`: This module randomly zeroes some of the elements of the input tensor with probability `p`. Check the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#dropout) for more details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMtPp5OeCakG",
        "pycharm": {}
      },
      "source": [
        "## Now we can start building our model\n",
        "## We first import the pytorch nn module\n",
        "import torch.nn as nn\n",
        "## Then define the model class\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(Net, self).__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(in_features=784, out_features=4096, bias=True),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(in_features=4096, out_features=4096, bias=True),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(in_features=4096, out_features=num_classes, bias=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # B x C x H x W -> B x C*H*W\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nayicPkJCkWy",
        "pycharm": {}
      },
      "source": [
        "## Initialization\n",
        "Please read the comments and understand the purpose of different lines of code."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model\n",
        "We will initialize the model, transfer to the desired device and set the parameters to receive gradients."
      ],
      "metadata": {
        "id": "QxrBZEY8vTLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the model, note the parameter is the number of class\n",
        "model = Net(10)\n",
        "# device: cpu or cuda (gpu)\n",
        "device = torch.device(\"cuda\")\n",
        "# map to device\n",
        "model = model.to(device) # `model.cuda()` will also do the same job\n",
        "# make all the parameters in the model trainable\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "nMaITZsPvfNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optimizer for Stochastic Gradient Descent\n",
        "For updating the parameters, PyTorch provides the package `torch.optim` that has most popular optimizers implemented. We will discuss the specific optimizers and their differences later in the course, but will for now use the simplest of them: `torch.optim.SGD`. Stochastic Gradient Descent updates parameters by multiplying the gradients with a small constant, called learning rate, and subtracting those from the parameters (hence minimizing the loss). Therefore, we slowly move towards the direction of minimizing the loss. A good default value of the learning rate for a small network as ours is 0.1."
      ],
      "metadata": {
        "id": "xpe9O7YVvXJR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjyEGZSdCk_i",
        "pycharm": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "# define optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The optimizer provides two useful functions: `optimizer.step()`, and `optimizer.zero_grad()`. The step function updates the parameters based on the gradients as explained above. The function `optimizer.zero_grad()` sets the gradients of all parameters to zero. While this function seems less relevant at first, it is a crucial pre-step before performing backpropagation. If we call the backward function on the loss while the parameter gradients are non-zero from the previous batch, the new gradients would actually be added to the previous ones instead of overwriting them. This is done because a parameter might occur multiple times in a computation graph, and we need to sum the gradients in this case instead of replacing them. Hence, remember to call `optimizer.zero_grad()` before calculating the gradients of a batch."
      ],
      "metadata": {
        "id": "dQU08NHBxEb8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oap6-7RKG0s7",
        "pycharm": {}
      },
      "source": [
        "## Average Meter\n",
        "It is a simple class for keeping training statistics, such as losses and accuracies etc. The `.val` field usually holds the statistics for the current batch, whereas the `.avg` field hold statistics for the current epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeLH7fbOHDhH",
        "pycharm": {}
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSk4VfO_C4tL",
        "pycharm": {}
      },
      "source": [
        "## Train and Test Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJeHMF_BC7bg",
        "pycharm": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "##define train function\n",
        "def train_one_epoch(model, device, train_loader, optimizer):\n",
        "    # average meter\n",
        "    loss = AverageMeter()\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    # use tqdm for showing the status\n",
        "    tk0 = tqdm(train_loader, total=int(len(train_loader)))\n",
        "\n",
        "    # fetching data from data loader\n",
        "    for batch_idx, (data, target) in enumerate(tk0):\n",
        "\n",
        "        # pass the data to the device\n",
        "        data, target = data.to(device), target.to(device)  # data, target = data.cuda(), target.cuda() will also do\n",
        "\n",
        "        # sets the gradients of all parameters to zero\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # compute the output by passing the input to the model\n",
        "        output = model(data) # model.forward(data) will also do the same thing\n",
        "\n",
        "        # compute cross entropy loss\n",
        "        loss_this = F.cross_entropy(output, target)\n",
        "\n",
        "        # perform backpropagation\n",
        "        loss_this.backward()\n",
        "\n",
        "        # update the parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # update the loss average meter\n",
        "        loss.update(loss_this.item(), target.shape[0])\n",
        "\n",
        "    print('Train: Average loss: {:.4f}\\n'.format(loss.avg))\n",
        "\n",
        "    return loss.avg\n",
        "\n",
        "##define test function\n",
        "def test(model, device, test_loader):\n",
        "    # average meters\n",
        "    loss = AverageMeter()\n",
        "    acc = AverageMeter()\n",
        "\n",
        "    # number of correctly classified instances\n",
        "    correct = 0\n",
        "\n",
        "    # switch to eval or test mode\n",
        "    model.eval()\n",
        "\n",
        "    # fetching data from data loader\n",
        "    for data, target in test_loader:\n",
        "\n",
        "        # pass the data to the device\n",
        "        data, target = data.to(device), target.to(device)  # data, target = data.cuda(), target.cuda() will also do\n",
        "\n",
        "        # since during testing, we don't need to compute gradient we compute\n",
        "        # the output as follows with `torch.no_grad()`. This will save memory/computation\n",
        "        with torch.no_grad():\n",
        "            output = model(data)\n",
        "\n",
        "        # compute cross entropy loss for checking overfitting or underfitting\n",
        "        loss_this = F.cross_entropy(output, target)\n",
        "\n",
        "        # get the index of the max log-probability\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "\n",
        "        # compute how many of them are correct\n",
        "        correct_this = pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "        # sum the total correct classifications\n",
        "        correct += correct_this\n",
        "\n",
        "        # accuracy for this batch\n",
        "        acc_this = correct_this/target.shape[0]*100.0\n",
        "\n",
        "        # update the accuracy and loss average meter\n",
        "        acc.update(acc_this, target.shape[0])\n",
        "        loss.update(loss_this.item(), target.shape[0])\n",
        "\n",
        "    print('Test: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        loss.avg, correct, len(test_loader.dataset), acc.avg))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzxivyrXDB7a",
        "pycharm": {}
      },
      "source": [
        "## Training Loop\n",
        "Training loop with logging of training loss. Perform testing after training the model for some epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIva4af2h_PZ",
        "pycharm": {}
      },
      "source": [
        "# import tensorboard logger from PyTorch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "# create TensorBoard logger\n",
        "writer = SummaryWriter('runs/mnist_experiment_1')\n",
        "# number of epochs we decide to train\n",
        "num_epoch = 5\n",
        "for epoch in range(1, num_epoch + 1):\n",
        "    epoch_loss = train_one_epoch(model, device, train_loader, optimizer)\n",
        "    writer.add_scalar('training_loss', epoch_loss, global_step = epoch)\n",
        "test(model, device, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training loss curve"
      ],
      "metadata": {
        "id": "grSrfsp5BhTC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The TensorBoard file in the folder runs/mnist_experiment_1 now contains a training loss curve over number of epochs. To start the TensorBoard visualizer, simply run the following statements."
      ],
      "metadata": {
        "id": "5FhhYHF_BUBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tensorboard extension for Jupyter Notebook, only need to start TB in the notebook\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs/mnist_experiment_1"
      ],
      "metadata": {
        "id": "oR9tsiHJ6LlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B19bDzBDIV4",
        "pycharm": {}
      },
      "source": [
        "## Summary\n",
        "Show the summary of the model. It shows the number of parameters in layerwise as well as the total number of parameters. It also shows the memories required for training the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eztTJ4VDKCA",
        "pycharm": {}
      },
      "source": [
        "from torchsummary import summary\n",
        "summary(model, (1, 28, 28))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}